from fastapi import FastAPI, HTTPException, Query, Request
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import datetime
import pytz
import logging
import traceback
import asyncio
import aiohttp
import requests
import tempfile
import os
import aiohttp
from bs4 import BeautifulSoup
import base64
import json
import time
import os
import sys
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from urllib.parse import urljoin

# Fix for Playwright on Windows:
# https://github.com/microsoft/playwright-python/issues/1342
if sys.platform == "win32":
    import asyncio
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

try:
    from config import NETWORK_CONFIG, CONTENT_CONFIG, SERVICE_CONFIG, LOG_CONFIG
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    NETWORK_CONFIG = {
        "timeout": {"connect": 15, "read": 60},
        "retries": {"max_attempts": 3, "delay_factor": 2},
        "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    CONTENT_CONFIG = {
        "max_length": {"default": 10000, "min": 100, "max": 50000},
        "parallel_limit": 10
    }
    SERVICE_CONFIG = {"host": "0.0.0.0", "port": 8001}
    LOG_CONFIG = {"level": "DEBUG"}

# é…ç½®æ—¥å¿—
log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f'visitapi_{log_timestamp}.log'

logging.getLogger().handlers.clear()

file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)

logger = logging.getLogger(__name__)
logger.info(f"Visit API starting up - Logging to file: {log_filename}")

app = FastAPI(
    title="Visit API - Page Access Service",
    version="1.0.0",
    description="""
    ## Visit API - ç½‘é¡µè®¿é—®æœåŠ¡

    åŸºäº Jina AI MCP æ¥å£è®¾è®¡çš„é¡µé¢è®¿é—®æœåŠ¡ï¼Œæä¾›ç½‘é¡µå†…å®¹æå–ã€åˆ†æå’Œå¤„ç†åŠŸèƒ½ã€‚

    ### ä¸»è¦åŠŸèƒ½
    * **read_url** - æå–ç½‘é¡µçš„å¹²å‡€å†…å®¹
    * **parallel_read** - å¹¶è¡Œè¯»å–å¤šä¸ªç½‘é¡µ
    * **screenshot** - ç½‘é¡µæˆªå›¾åŠŸèƒ½ 
    * **analyze_datetime** - åˆ†æç½‘é¡µå‘å¸ƒ/æ›´æ–°æ—¶é—´
    * **search** - ç½‘ç»œæœç´¢åŠŸèƒ½
    * **health** - æœåŠ¡å¥åº·æ£€æŸ¥
    * **primer** - ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯

    ### ä½¿ç”¨æ–¹æ³•
    1. åœ¨ä¸‹æ–¹é€‰æ‹©è¦æµ‹è¯•çš„ API ç«¯ç‚¹
    2. å¡«å†™å¿…è¦çš„å‚æ•°
    3. ç‚¹å‡» "Try it out" æŒ‰é’®
    4. ç‚¹å‡» "Execute" æ‰§è¡Œè¯·æ±‚
    5. æŸ¥çœ‹è¿”å›ç»“æœ

    ### æ”¯æŒçš„ç½‘ç«™ç±»å‹
    - æ–°é—»ç½‘ç«™
    - åšå®¢æ–‡ç« 
    - ç»´åŸºç™¾ç§‘
    - çŸ¥ä¹æ–‡ç« 
    - æŠ€æœ¯æ–‡æ¡£
    - å¤§å¤šæ•°é™æ€ç½‘é¡µ

    """,
    contact={
        "name": "Visit API Support",
        "email": "support@visitapi.com",
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT",
    },
)

# åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–Playwright
@app.on_event("startup")
async def startup_event():
    logger.info("Starting up Playwright...")
    app.state.playwright = await async_playwright().start()
    proxy_config = NETWORK_CONFIG.get("proxy", {})
    proxy_url = proxy_config.get("https") or proxy_config.get("http")
    
    launch_options = {
        "headless": True,
        "args": [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-accelerated-2d-canvas',
            '--no-first-run',
            '--no-zygote',
            '--disable-gpu'
        ]
    }
    
    if proxy_url:
        logger.info(f"Playwright using proxy: {proxy_url}")
        launch_options["proxy"] = {"server": proxy_url}
        
    app.state.browser = await app.state.playwright.chromium.launch(**launch_options)
    logger.info("Playwright started successfully.")

# åº”ç”¨å…³é—­æ—¶æ¸…ç†Playwrightèµ„æº
@app.on_event("shutdown")
async def shutdown_event():
    logger.info("Shutting down Playwright...")
    if hasattr(app.state, 'browser') and app.state.browser.is_connected():
        await app.state.browser.close()
    if hasattr(app.state, 'playwright'):
        await app.state.playwright.stop()
    logger.info("Playwright shut down successfully.")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class ReadUrlRequest(BaseModel):
    url: str
    include_images: Optional[bool] = False
    include_links: Optional[bool] = False
    max_content_length: Optional[int] = 10000

    model_config = {
        "json_schema_extra": {
            "example": {
                "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
                "include_images": True,
                "include_links": True,
                "max_content_length": 5000
            }
        }
    }

class ScreenshotRequest(BaseModel):
    url: str
    width: Optional[int] = 1280
    height: Optional[int] = 720
    full_page: Optional[bool] = False
    format: Optional[str] = "png"

    model_config = {
        "json_schema_extra": {
            "example": {
                "url": "https://www.example.com",
                "width": 1280,
                "height": 720,
                "full_page": False,
                "format": "png"
            }
        }
    }

class ParallelReadRequest(BaseModel):
    urls: List[str]
    include_images: Optional[bool] = False
    include_links: Optional[bool] = False
    max_content_length: Optional[int] = 10000

    model_config = {
        "json_schema_extra": {
            "example": {
                "urls": [
                    "https://en.wikipedia.org/wiki/Machine_learning",
                    "https://en.wikipedia.org/wiki/Deep_learning",
                    "https://en.wikipedia.org/wiki/Natural_language_processing"
                ],
                "include_images": False,
                "include_links": True,
                "max_content_length": 3000
            }
        }
    }

class SearchRequest(BaseModel):
    query: str
    max_results: Optional[int] = 10
    language: Optional[str] = "en"

    model_config = {
        "json_schema_extra": {
            "example": {
                "query": "artificial intelligence latest news",
                "max_results": 10,
                "language": "en"
            }
        }
    }

class DatetimeAnalysisRequest(BaseModel):
    url: str

    model_config = {
        "json_schema_extra": {
            "example": {
                "url": "https://news.ycombinator.com/item?id=123456"
            }
        }
    }

# å“åº”æ¨¡å‹
class PageContent(BaseModel):
    url: str
    title: Optional[str] = None
    content: Optional[str] = None
    clean_text: Optional[str] = None
    meta_info: Optional[Dict[str, Any]] = None
    images: Optional[List[Dict[str, str]]] = None
    links: Optional[List[Dict[str, str]]] = None
    extraction_status: str
    error: Optional[str] = None
    timestamp: str

class ScreenshotResponse(BaseModel):
    url: str
    screenshot_base64: Optional[str] = None
    width: int
    height: int
    format: str
    error: Optional[str] = None
    timestamp: str

class DatetimeInfo(BaseModel):
    url: str
    published_date: Optional[str] = None
    updated_date: Optional[str] = None
    detected_dates: Optional[List[str]] = None
    confidence: Optional[float] = None
    method: Optional[str] = None
    error: Optional[str] = None

class ApiResponse(BaseModel):
    code: int = 200
    log_id: str
    msg: Optional[str] = None
    data: Union[Dict[str, Any], List[Dict[str, Any]], Any]
    timestamp: str

# æ—¥å¿—ä¸­é—´ä»¶
@app.middleware("http")
async def log_requests(request, call_next):
    start_time = time.time()
    logger.info(f"Request started: {request.method} {request.url}")
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(f"Request completed: {request.method} {request.url} - Status: {response.status_code} - Time: {process_time:.3f}s")
    
    return response

# å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global error handler caught: {str(exc)}")
    logger.error(traceback.format_exc())
    return JSONResponse(
        status_code=500,
        content={
            "code": 500,
            "log_id": datetime.now().strftime('%Y%m%d%H%M%S'),
            "msg": str(exc),
            "data": None,
            "timestamp": datetime.now(pytz.UTC).isoformat(),
            "traceback": traceback.format_exc() if logging.getLogger().level == logging.DEBUG else None
        }
    )

# å·¥å…·å‡½æ•°
def format_timestamp() -> str:
    """è¿”å›UTCæ—¶é—´æˆ³"""
    return datetime.now(pytz.UTC).isoformat()

async def extract_content_with_playwright(browser, url: str, include_images: bool, include_links: bool, max_length: int) -> PageContent:
    """ä½¿ç”¨Playwrightæå–å†…å®¹ï¼ˆé€‚ç”¨äºéœ€è¦JSæ¸²æŸ“æˆ–åçˆ¬è™«å¼ºçš„ç½‘ç«™ï¼‰"""
    logger.info(f"Using shared Playwright browser for {url}")
    
    # ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»ºç‹¬ç«‹çš„æµè§ˆå™¨ä¸Šä¸‹æ–‡
    context = await browser.new_context()
    page = await context.new_page()
    
    try:
        # å¢åŠ é¡µé¢åŠ è½½è¶…æ—¶
        await page.goto(url, timeout=NETWORK_CONFIG["timeout"].get("read", 60) * 1000, wait_until='domcontentloaded')
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©åŠ¨æ€å†…å®¹åŠ è½½
        await asyncio.sleep(3) 
        
        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        return process_soup_content(soup, url, include_images, include_links, max_length)
        
    except PlaywrightTimeoutError:
        logger.error(f"Playwright timed out loading page: {url}")
        return f"Error: Page loading timed out for {url}. This might be due to network issues or the page taking too long to load."
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Playwright error for {url}: {error_msg}")
        
        # ç‰¹æ®Šå¤„ç†å¸¸è§é”™è¯¯
        if "net::ERR_ABORTED" in error_msg:
            if url.endswith('.pdf'):
                return f"Error: Cannot access PDF directly at {url}. PDF files may require special handling or may be blocked by the server."
            else:
                return f"Error: Access to {url} was aborted. This may be due to server restrictions, proxy issues, or anti-bot measures."
        elif "net::ERR_FAILED" in error_msg:
            return f"Error: Failed to connect to {url}. The server may be down or unreachable."
        elif "net::ERR_TIMED_OUT" in error_msg:
            return f"Error: Connection to {url} timed out. Please try again later."
        else:
            return f"Error: Unable to access {url}. Reason: {error_msg}"
    finally:
        await context.close()  # å…³é—­ä¸Šä¸‹æ–‡ä¼šè‡ªåŠ¨å…³é—­å…¶ä¸­çš„æ‰€æœ‰é¡µé¢

def process_soup_content(soup, url: str, include_images: bool, include_links: bool, max_length: int) -> PageContent:
    """å¤„ç†BeautifulSoupè§£æåçš„å†…å®¹"""
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):
        tag.decompose()
    
    # æå–æ ‡é¢˜
    title = soup.title.string.strip() if soup.title else None
    
    # æå–å…ƒæ•°æ®
    meta_info = {
        'description': soup.find('meta', {'name': 'description'})['content'] if soup.find('meta', {'name': 'description'}) else None,
        'keywords': soup.find('meta', {'name': 'keywords'})['content'] if soup.find('meta', {'name': 'keywords'}) else None,
        'author': soup.find('meta', {'name': 'author'})['content'] if soup.find('meta', {'name': 'author'}) else None,
        'language': soup.find('html', {'lang': True})['lang'] if soup.find('html', {'lang': True}) else None,
        'charset': soup.find('meta', {'charset': True})['charset'] if soup.find('meta', {'charset': True}) else None,
    }
            
    
    # æå–ä¸»è¦å†…å®¹
    content = ""
    
    # å°è¯•æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
    main_selectors = [
        'main', 'article', '[role="main"]',
        '.content', '.main-content', '.article-content',
        '.post-content', '.entry-content', '#content'
    ]
    
    for selector in main_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            content = main_content.get_text(separator=' ', strip=True)
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œæå–bodyå†…å®¹
    if not content:
        body = soup.find('body')
        if body:
            content = body.get_text(separator=' ', strip=True)
    
    # æ¸…ç†å’Œé™åˆ¶å†…å®¹é•¿åº¦
    clean_text = ' '.join(content.split())
    if len(clean_text) > max_length:
        clean_text = clean_text[:max_length] + '...'
    
    # æå–å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    images = []
    if include_images:
        for img in soup.find_all('img', src=True):
            src = img['src']
            if not src.startswith(('http://', 'https://')):
                src = urljoin(url, src)
            images.append({
                'src': src,
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
    
    # æå–é“¾æ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
    links = []
    if include_links:
        for link in soup.find_all('a', href=True):
            href = link['href']
            if not href.startswith(('http://', 'https://')):
                href = urljoin(url, href)
            links.append({
                'href': href,
                'text': link.get_text(strip=True),
                'title': link.get('title', '')
            })
    
    return PageContent(
        url=url,
        title=title,
        content=content[:max_length] if content else None,
        clean_text=clean_text,
        meta_info=meta_info,
        images=images[:20] if images else None,  # é™åˆ¶å›¾ç‰‡æ•°é‡
        links=links[:50] if links else None,    # é™åˆ¶é“¾æ¥æ•°é‡
        extraction_status='success',
        timestamp=format_timestamp()
    )

async def extract_pdf_content(url: str, max_length: int = 10000) -> PageContent:
    """å°è¯•æå–PDFå†…å®¹çš„å¤šç§æ–¹æ³•"""
    logger.info(f"Attempting to extract PDF content from: {url}")
    
    # æ–¹æ³•1: å°è¯•é€šè¿‡requestsç›´æ¥ä¸‹è½½
    try:
        proxy_config = NETWORK_CONFIG.get("proxy", {})
        proxies = {}
        if proxy_config.get("http"):
            proxies["http"] = proxy_config["http"]
        if proxy_config.get("https"):
            proxies["https"] = proxy_config["https"]
            
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, proxies=proxies, timeout=30, stream=True)
        response.raise_for_status()
        
        # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯PDF
        content_type = response.headers.get('content-type', '').lower()
        if 'pdf' not in content_type and not url.endswith('.pdf'):
            logger.warning(f"URL {url} may not be a PDF file")
        
        # å°è¯•å®‰è£…å’Œä½¿ç”¨PyPDF2æ¥è§£æPDF
        try:
            import PyPDF2
            import io
            
            # ä¸‹è½½PDFå†…å®¹
            pdf_content = response.content
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))
            
            text_content = ""
            for page_num in range(min(len(pdf_reader.pages), 10)):  # é™åˆ¶å‰10é¡µ
                page = pdf_reader.pages[page_num]
                text_content += page.extract_text() + "\n"
            
            # æ¸…ç†å’Œæˆªæ–­å†…å®¹
            text_content = text_content.strip()
            if len(text_content) > max_length:
                text_content = text_content[:max_length] + "..."
            
            return PageContent(
                url=url,
                title=f"PDF Document: {url.split('/')[-1]}",
                content=text_content,
                extraction_status='success',
                timestamp=format_timestamp()
            )
            
        except ImportError:
            logger.warning("PyPDF2 not installed. Install with: pip install PyPDF2")
            return PageContent(
                url=url,
                extraction_status='error',
                error="PDF processing requires PyPDF2 library. Please install with: pip install PyPDF2",
                timestamp=format_timestamp()
            )
        except Exception as pdf_error:
            logger.error(f"PDF parsing error: {pdf_error}")
            return PageContent(
                url=url,
                title=f"PDF Document: {url.split('/')[-1]}",
                content=f"PDF file detected but content extraction failed. File size: {len(response.content)} bytes",
                extraction_status='partial',
                error=f"PDF parsing failed: {str(pdf_error)}",
                timestamp=format_timestamp()
            )
            
    except Exception as e:
        logger.error(f"Failed to download/process PDF from {url}: {e}")
        return PageContent(
            url=url,
            extraction_status='error',
            error=f"Failed to access PDF: {str(e)}",
            timestamp=format_timestamp()
        )

async def extract_clean_content(browser, url: str, include_images: bool = False, include_links: bool = False, max_length: int = 10000) -> PageContent:
    """æå–ç½‘é¡µçš„å¹²å‡€å†…å®¹"""
    logger.debug(f"Starting content extraction for URL: {url}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæ–‡ä»¶
    if url.endswith('.pdf') or 'pdf' in url.lower():
        logger.info(f"Detected PDF URL: {url}, using PDF extraction method")
        return await extract_pdf_content(url, max_length)
    
    if not url or not url.startswith(('http://', 'https://')):
        return PageContent(
            url=url,
            extraction_status='error',
            error='Invalid URL format',
            timestamp=format_timestamp()
        )
    
    try:
        # ç›´æ¥ä½¿ç”¨ Playwright è¿›è¡Œå†…å®¹æå–
        result = await extract_content_with_playwright(browser, url, include_images, include_links, max_length)
        
        # æ£€æŸ¥æ˜¯å¦è¿”å›é”™è¯¯ä¿¡æ¯
        if isinstance(result, str) and result.startswith("Error:"):
            return PageContent(
                url=url,
                extraction_status='error',
                error=result,
                timestamp=format_timestamp()
            )
        else:
            return result
            
    except Exception as e:
        logger.error(f"Error extracting content from {url}: {str(e)}")
        logger.error(traceback.format_exc())
        return PageContent(
            url=url,
            extraction_status='error',
            error=str(e),
            timestamp=format_timestamp()
        )

async def capture_screenshot(browser, url: str, width: int = 1280, height: int = 720, full_page: bool = False, format: str = "png") -> ScreenshotResponse:
    """ä½¿ç”¨Playwrightæ•è·ç½‘é¡µæˆªå›¾"""
    logger.debug(f"Screenshot request for URL: {url}")
    
    context = await browser.new_context()
    page = await context.new_page()
    await page.set_viewport_size({"width": width, "height": height})
    
    try:
        await page.goto(url, timeout=NETWORK_CONFIG["timeout"].get("read", 60) * 1000)
        
        screenshot_bytes = await page.screenshot(
            path=None, 
            full_page=full_page,
            type=format if format in ['png', 'jpeg'] else 'png'
        )
        
        screenshot_base64 = base64.b64encode(screenshot_bytes).decode('utf-8')
        
        return ScreenshotResponse(
            url=url,
            screenshot_base64=screenshot_base64,
            width=width,
            height=height,
            format=format,
            timestamp=format_timestamp()
        )
    except Exception as e:
        logger.error(f"Error capturing screenshot for {url}: {str(e)}")
        return ScreenshotResponse(
            url=url,
            error=str(e),
            width=width,
            height=height,
            format=format,
            timestamp=format_timestamp()
        )
    finally:
        await context.close()

async def analyze_page_datetime(browser, url: str) -> DatetimeInfo:
    """åˆ†æç½‘é¡µçš„å‘å¸ƒ/æ›´æ–°æ—¶é—´"""
    logger.debug(f"Analyzing datetime for URL: {url}")
    
    try:
        # ä½¿ç”¨ Playwright è·å–é¡µé¢å†…å®¹è¿›è¡Œåˆ†æ
        context = await browser.new_context()
        page = await context.new_page()
        await page.goto(url, timeout=NETWORK_CONFIG["timeout"].get("read", 60) * 1000)
        html_content = await page.content()
        await context.close()

        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾å„ç§æ—¶é—´æ ‡è®°
        published_date = None
        updated_date = None
        detected_dates = []
        
        # æŸ¥æ‰¾å…ƒæ•°æ®ä¸­çš„æ—¶é—´
        date_metas = [
            'article:published_time', 'article:modified_time',
            'datePublished', 'dateModified', 'pubdate'
        ]
        
        for meta_name in date_metas:
            meta = soup.find('meta', {'property': meta_name}) or soup.find('meta', {'name': meta_name})
            if meta and meta.get('content'):
                date_str = meta['content']
                detected_dates.append(date_str)
                if 'published' in meta_name.lower() and not published_date:
                    published_date = date_str
                elif 'modified' in meta_name.lower() and not updated_date:
                    updated_date = date_str
        
        # æŸ¥æ‰¾timeæ ‡ç­¾
        for time_tag in soup.find_all('time'):
            datetime_attr = time_tag.get('datetime')
            if datetime_attr:
                detected_dates.append(datetime_attr)
                if not published_date:
                    published_date = datetime_attr
        
        # ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—
        confidence = min(1.0, len(detected_dates) * 0.3)
        
        return DatetimeInfo(
            url=url,
            published_date=published_date,
            updated_date=updated_date,
            detected_dates=detected_dates,
            confidence=confidence,
            method='meta_and_time_tags'
        )
                
    except Exception as e:
        logger.error(f"Error analyzing datetime for {url}: {str(e)}")
        return DatetimeInfo(
            url=url,
            error=str(e)
        )

# API ç«¯ç‚¹

@app.get("/test", 
         response_class=HTMLResponse,
         summary="æµ‹è¯•é¡µé¢",
         description="æä¾›ä¸€ä¸ªäº¤äº’å¼çš„æµ‹è¯•ç•Œé¢ï¼Œå¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æµ‹è¯•æ‰€æœ‰ API åŠŸèƒ½",
         tags=["æµ‹è¯•å·¥å…·"])
async def test_page():
    """è¿”å›æµ‹è¯•é¡µé¢"""
    try:
        with open("test.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse("""
        <html>
            <body style="font-family: Arial; text-align: center; padding: 50px;">
                <h1>æµ‹è¯•é¡µé¢ä¸å­˜åœ¨</h1>
                <p>è¯·ç¡®ä¿ test.html æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­</p>
                <p><a href="/docs">ç‚¹å‡»è¿™é‡Œè®¿é—® API æ–‡æ¡£</a></p>
            </body>
        </html>
        """)

@app.get("/",
         response_class=HTMLResponse,
         summary="æœåŠ¡é¦–é¡µ",
         description="Visit API æœåŠ¡çš„æ¬¢è¿é¡µé¢ï¼Œæä¾›æœåŠ¡æ¦‚è§ˆå’Œå¿«é€Ÿé“¾æ¥",
         tags=["ç³»ç»Ÿ"])
async def root():
    """æ ¹ç«¯ç‚¹ - æœåŠ¡ä¿¡æ¯"""
    return HTMLResponse("""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Visit API - ç½‘é¡µè®¿é—®æœåŠ¡</title>
        <style>
            body {
                font-family: 'Segoe UI', Arial, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                margin: 0;
                padding: 50px 20px;
                color: white;
                text-align: center;
                min-height: 100vh;
                display: flex;
                flex-direction: column;
                justify-content: center;
            }
            .container {
                max-width: 800px;
                margin: 0 auto;
                background: rgba(255,255,255,0.1);
                padding: 50px;
                border-radius: 20px;
                backdrop-filter: blur(10px);
                box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            }
            h1 {
                font-size: 3em;
                margin-bottom: 20px;
                text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            }
            .subtitle {
                font-size: 1.3em;
                margin-bottom: 40px;
                opacity: 0.9;
            }
            .features {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 20px;
                margin: 40px 0;
            }
            .feature {
                background: rgba(255,255,255,0.1);
                padding: 20px;
                border-radius: 10px;
                border: 1px solid rgba(255,255,255,0.2);
            }
            .links {
                margin-top: 40px;
            }
            .btn {
                display: inline-block;
                background: rgba(255,255,255,0.2);
                color: white;
                padding: 15px 30px;
                margin: 10px;
                border-radius: 50px;
                text-decoration: none;
                border: 2px solid rgba(255,255,255,0.3);
                transition: all 0.3s;
                font-weight: 500;
            }
            .btn:hover {
                background: rgba(255,255,255,0.3);
                transform: translateY(-2px);
                box-shadow: 0 10px 20px rgba(0,0,0,0.2);
            }
            .status {
                background: rgba(76, 175, 80, 0.2);
                padding: 10px 20px;
                border-radius: 20px;
                display: inline-block;
                margin-bottom: 20px;
                border: 1px solid rgba(76, 175, 80, 0.5);
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="status">ğŸŸ¢ æœåŠ¡è¿è¡Œä¸­</div>
            <h1>ğŸŒ Visit API</h1>
            <p class="subtitle">ç½‘é¡µè®¿é—®æœåŠ¡ - å†…å®¹æå–ã€åˆ†æå’Œå¤„ç†</p>
            
            <div class="features">
                <div class="feature">
                    <h3>ğŸ” å†…å®¹æå–</h3>
                    <p>æ™ºèƒ½æå–ç½‘é¡µæ ¸å¿ƒå†…å®¹</p>
                </div>
                <div class="feature">
                    <h3>âš¡ å¹¶è¡Œå¤„ç†</h3>
                    <p>åŒæ—¶å¤„ç†å¤šä¸ªç½‘é¡µ</p>
                </div>
                <div class="feature">
                    <h3>ğŸ•’ æ—¶é—´åˆ†æ</h3>
                    <p>åˆ†æç½‘é¡µæ—¶é—´ä¿¡æ¯</p>
                </div>
                <div class="feature">
                    <h3>ğŸ“Š æ•°æ®ç»“æ„åŒ–</h3>
                    <p>è¿”å›ç»“æ„åŒ–çš„æ•°æ®</p>
                </div>
            </div>
            
            <div class="links">
                <a href="/test" class="btn">ğŸ§ª äº¤äº’å¼æµ‹è¯•</a>
                <a href="/docs" class="btn">ğŸ“– API æ–‡æ¡£</a>
                <a href="/health" class="btn">ğŸ’Š å¥åº·æ£€æŸ¥</a>
                <a href="/primer" class="btn">â„¹ï¸ ç³»ç»Ÿä¿¡æ¯</a>
            </div>
            
            <p style="margin-top: 40px; opacity: 0.7;">
                Visit API v1.0.0 | åŸºäº Jina MCP æ¥å£è®¾è®¡
            </p>
        </div>
    </body>
    </html>
    """)

@app.post("/read_url", 
          response_model=ApiResponse,
          summary="æå–ç½‘é¡µå†…å®¹",
          description="""
          æå–æŒ‡å®šç½‘é¡µçš„å¹²å‡€å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ­£æ–‡ã€å…ƒæ•°æ®ç­‰ä¿¡æ¯ã€‚
          
          **æ”¯æŒçš„åŠŸèƒ½ï¼š**
          - è‡ªåŠ¨è¯†åˆ«ä¸»è¦å†…å®¹åŒºåŸŸ
          - æ¸…ç†æ— ç”¨çš„æ ‡ç­¾å’Œè„šæœ¬
          - æå–å›¾ç‰‡å’Œé“¾æ¥ï¼ˆå¯é€‰ï¼‰
          - æ”¯æŒå¤šç§å­—ç¬¦ç¼–ç 
          - å¤„ç†å„ç§ç½‘ç«™ç±»å‹
          
          **ç¤ºä¾‹ç½‘ç«™ï¼š**
          - https://en.wikipedia.org/wiki/Artificial_intelligence
          - https://www.zhihu.com/question/xxxxx
          - https://news.ycombinator.com/item?id=xxxxx
          """,
          tags=["å†…å®¹æå–"])
async def read_url(request: ReadUrlRequest, http_request: Request):
    """æå–ç½‘é¡µå†…å®¹"""
    logger.info(f"Reading URL: {request.url}")
    
    try:
        browser = http_request.app.state.browser
        content = await extract_clean_content(
            browser=browser,
            url=request.url,
            include_images=request.include_images,
            include_links=request.include_links,
            max_length=request.max_content_length
        )
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Success",
            data=content.model_dump(),
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error reading URL {request.url}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/parallel_read", 
          response_model=ApiResponse,
          summary="å¹¶è¡Œè¯»å–å¤šä¸ªç½‘é¡µ",
          description="""
          åŒæ—¶è¯»å–å¤šä¸ªç½‘é¡µçš„å†…å®¹ï¼Œé€‚ç”¨äºéœ€è¦å¿«é€Ÿè·å–å¤šä¸ªé¡µé¢ä¿¡æ¯çš„åœºæ™¯ã€‚
          
          **ç‰¹ç‚¹ï¼š**
          - å¹¶å‘å¤„ç†ï¼Œæ˜¾è‘—æé«˜æ•ˆç‡
          - è‡ªåŠ¨å¤„ç†å¤±è´¥çš„è¯·æ±‚
          - æ”¯æŒæœ€å¤šåŒæ—¶å¤„ç† 10 ä¸ªç½‘é¡µ
          - è¿”å›æ‰€æœ‰ç»“æœï¼ˆåŒ…æ‹¬å¤±è´¥çš„ï¼‰
          
          **ä½¿ç”¨åœºæ™¯ï¼š**
          - æ–°é—»èšåˆ
          - å†…å®¹æ¯”è¾ƒ
          - æ‰¹é‡æ•°æ®æ”¶é›†
          """,
          tags=["å†…å®¹æå–"])
async def parallel_read(request: ParallelReadRequest, http_request: Request):
    """å¹¶è¡Œè¯»å–å¤šä¸ªç½‘é¡µ"""
    logger.info(f"Parallel reading {len(request.urls)} URLs")
    
    try:
        # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
        max_parallel = CONTENT_CONFIG["parallel_limit"]
        if len(request.urls) > max_parallel:
            raise HTTPException(
                status_code=400, 
                detail=f"æœ€å¤šæ”¯æŒåŒæ—¶è¯»å– {max_parallel} ä¸ªç½‘é¡µ"
            )
        
        browser = http_request.app.state.browser
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for url in request.urls:
            task = extract_clean_content(
                browser=browser,
                url=url,
                include_images=request.include_images,
                include_links=request.include_links,
                max_length=request.max_content_length
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "url": request.urls[i],
                    "extraction_status": "error",
                    "error": str(result),
                    "timestamp": format_timestamp()
                })
            else:
                processed_results.append(result.model_dump())
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Parallel read completed",
            data=processed_results,
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error in parallel read: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/screenshot", 
          response_model=ApiResponse,
          summary="ç½‘é¡µæˆªå›¾",
          description="""
          æ•è·ç½‘é¡µçš„æˆªå›¾å›¾åƒã€‚
          
          **æ³¨æ„ï¼š** å½“å‰ä¸ºæ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éƒ¨ç½²éœ€è¦é›†æˆæµè§ˆå™¨å¼•æ“ï¼ˆå¦‚ Playwright æˆ– Seleniumï¼‰ã€‚
          
          **å‚æ•°è¯´æ˜ï¼š**
          - width/height: æˆªå›¾å°ºå¯¸
          - full_page: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢
          - format: å›¾ç‰‡æ ¼å¼ (png/jpeg)
          
          **å®é™…å®ç°éœ€è¦ï¼š**
          ```bash
          pip install playwright
          playwright install chromium
          ```
          """,
          tags=["å›¾åƒå¤„ç†"])
async def screenshot(request: ScreenshotRequest, http_request: Request):
    """ç½‘é¡µæˆªå›¾ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"Screenshot request for: {request.url}")
    
    try:
        browser = http_request.app.state.browser
        screenshot_result = await capture_screenshot(
            browser=browser,
            url=request.url,
            width=request.width,
            height=request.height,
            full_page=request.full_page,
            format=request.format
        )
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Screenshot completed (mock)",
            data=screenshot_result.model_dump(),
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error capturing screenshot: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze_datetime", 
          response_model=ApiResponse,
          summary="åˆ†æç½‘é¡µæ—¶é—´ä¿¡æ¯",
          description="""
          åˆ†æç½‘é¡µçš„å‘å¸ƒæ—¶é—´å’Œæ›´æ–°æ—¶é—´ã€‚
          
          **åˆ†ææ–¹æ³•ï¼š**
          - æå– HTML meta æ ‡ç­¾ä¸­çš„æ—¶é—´ä¿¡æ¯
          - æŸ¥æ‰¾ `<time>` æ ‡ç­¾çš„ datetime å±æ€§
          - æ£€æµ‹å¸¸è§çš„æ—¶é—´æ ¼å¼
          - è®¡ç®—æ£€æµ‹ç½®ä¿¡åº¦
          
          **é€‚ç”¨ç½‘ç«™ï¼š**
          - æ–°é—»ç½‘ç«™
          - åšå®¢æ–‡ç« 
          - æŠ€æœ¯æ–‡æ¡£
          - ç¤¾äº¤åª’ä½“å¸–å­
          """,
          tags=["æ•°æ®åˆ†æ"])
async def analyze_datetime(request: DatetimeAnalysisRequest, http_request: Request):
    """åˆ†æç½‘é¡µçš„å‘å¸ƒ/æ›´æ–°æ—¶é—´"""
    logger.info(f"Analyzing datetime for: {request.url}")
    
    try:
        browser = http_request.app.state.browser
        datetime_info = await analyze_page_datetime(browser, request.url)
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Datetime analysis completed",
            data=datetime_info.model_dump(),
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error analyzing datetime: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search", 
          response_model=ApiResponse,
          summary="ç½‘ç»œæœç´¢",
          description="""
          æ‰§è¡Œç½‘ç»œæœç´¢å¹¶è¿”å›ç»“æœã€‚
          
          **æ³¨æ„ï¼š** å½“å‰ä¸ºæ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éƒ¨ç½²éœ€è¦é›†æˆæœç´¢å¼•æ“ APIã€‚
          
          **å¯é›†æˆçš„æœç´¢å¼•æ“ï¼š**
          - DuckDuckGo (å…è´¹)
          - Google Custom Search API
          - Bing Search API
          - SerpAPI
          
          **å®é™…å®ç°éœ€è¦ï¼š**
          ```bash
          pip install duckduckgo-search playwright
          playwright install
          ```
          """,
          tags=["æœç´¢"])
async def search_web(request: SearchRequest):
    """ç½‘ç»œæœç´¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"Search request for: {request.query}")
    
    # è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éœ€è¦é›†æˆæœç´¢å¼•æ“API
    mock_results = [
        {
            "title": f"æœç´¢ç»“æœ 1: {request.query}",
            "url": "https://example.com/search-result-1",
            "snippet": f"è¿™æ˜¯å…³äº '{request.query}' çš„ç¬¬ä¸€ä¸ªæœç´¢ç»“æœæ‘˜è¦...",
            "rank": 1
        },
        {
            "title": f"æœç´¢ç»“æœ 2: {request.query}",
            "url": "https://example.com/search-result-2", 
            "snippet": f"è¿™æ˜¯å…³äº '{request.query}' çš„ç¬¬äºŒä¸ªæœç´¢ç»“æœæ‘˜è¦...",
            "rank": 2
        }
    ]
    
    return ApiResponse(
        code=200,
        log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
        msg="Search completed (mock)",
        data={
            "query": request.query,
            "results": mock_results[:request.max_results],
            "total_results": len(mock_results),
            "language": request.language
        },
        timestamp=format_timestamp()
    )

@app.get("/health",
         summary="å¥åº·æ£€æŸ¥",
         description="æ£€æŸ¥æœåŠ¡çš„è¿è¡ŒçŠ¶æ€å’Œç³»ç»Ÿå¥åº·çŠ¶å†µ",
         tags=["ç³»ç»Ÿ"])
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    logger.info("Health check endpoint accessed")
    
    # æ£€æŸ¥ä»£ç†çŠ¶æ€
    proxy_config = NETWORK_CONFIG.get("proxy", {})
    proxy_enabled = proxy_config.get("enabled", False)
    proxy_url = None
    if proxy_enabled:
        proxy_url = proxy_config.get("https") or proxy_config.get("http")
    
    health_status = {
        "status": "healthy",
        "service": "Visit API",
        "version": "1.0.0",
        "timestamp": format_timestamp(),
        "proxy_enabled": proxy_enabled,
        "proxy_url": proxy_url if proxy_enabled else None,
        "checks": {
            "logging": "ok",
            "memory": "ok",
            "disk_space": "ok",
            "proxy": "enabled" if proxy_enabled else "disabled"
        }
    }
    
    return health_status

@app.get("/primer",
         summary="ç³»ç»Ÿä¿¡æ¯",
         description="""
         æä¾›ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³ã€ç½‘ç»œä¿¡æ¯ã€æœåŠ¡èƒ½åŠ›ç­‰ã€‚
         
         **è¿”å›ä¿¡æ¯ï¼š**
         - å½“å‰æ—¶é—´æˆ³å’Œæ—¶åŒº
         - ç³»ç»Ÿä¸»æœºåå’ŒIP
         - å¹³å°å’ŒPythonç‰ˆæœ¬
         - æœåŠ¡ç‰ˆæœ¬å’Œèƒ½åŠ›åˆ—è¡¨
         """,
         tags=["ç³»ç»Ÿ"])
async def primer():
    """æä¾›ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯"""
    import socket
    import platform
    
    try:
        hostname = socket.gethostname()
        local_ip = socket.gethostbyname(hostname)
    except:
        hostname = "unknown"
        local_ip = "unknown"
    
    return {
        "timestamp": format_timestamp(),
        "timezone": str(pytz.UTC),
        "system_info": {
            "hostname": hostname,
            "local_ip": local_ip,
            "platform": platform.system(),
            "python_version": platform.python_version()
        },
        "client_context": {
            "service": "Visit API",
            "version": "1.0.0",
            "capabilities": [
                "read_url", "parallel_read", "screenshot", 
                "analyze_datetime", "search", "health_check"
            ]
        }
    }

if __name__ == "__main__":
    import uvicorn
    
    logger.info("Starting Visit API server...")
    logger.info(f"Log file: {log_filename}")
    
    uvicorn.run(
        "app:app",
        host=SERVICE_CONFIG.get("host", "0.0.0.0"),
        port=SERVICE_CONFIG.get("port", 8002),
        log_level="info",
        reload=False # reload=True might cause issues with the asyncio policy
    )
