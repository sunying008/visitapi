from fastapi import FastAPI, HTTPException, Query
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, field_validator
from typing import List, Optional, Dict, Any, Union
from datetime import datetime
import pytz
import logging
import traceback
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import base64
import json
import time
import os
try:
    from config import NETWORK_CONFIG, CONTENT_CONFIG, SERVICE_CONFIG, LOG_CONFIG
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    NETWORK_CONFIG = {
        "timeout": {"total": 60, "connect": 15},
        "retries": {"max_attempts": 3, "delay_factor": 2},
        "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    CONTENT_CONFIG = {
        "max_length": {"default": 10000, "min": 100, "max": 50000},
        "parallel_limit": 10
    }
    SERVICE_CONFIG = {"host": "0.0.0.0", "port": 8001}
    LOG_CONFIG = {"level": "DEBUG"}

# é…ç½®æ—¥å¿—
log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_filename = f'visitapi_{log_timestamp}.log'

logging.getLogger().handlers.clear()

file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.DEBUG)
console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))

root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)

logger = logging.getLogger(__name__)
logger.info(f"Visit API starting up - Logging to file: {log_filename}")

app = FastAPI(
    title="Visit API - Page Access Service",
    version="1.0.0",
    description="""
    ## Visit API - ç½‘é¡µè®¿é—®æœåŠ¡

    åŸºäº Jina AI MCP æ¥å£è®¾è®¡çš„é¡µé¢è®¿é—®æœåŠ¡ï¼Œæä¾›ç½‘é¡µå†…å®¹æå–ã€åˆ†æå’Œå¤„ç†åŠŸèƒ½ã€‚

    ### ä¸»è¦åŠŸèƒ½
    * **read_url** - æå–ç½‘é¡µçš„å¹²å‡€å†…å®¹
    * **parallel_read** - å¹¶è¡Œè¯»å–å¤šä¸ªç½‘é¡µ
    * **screenshot** - ç½‘é¡µæˆªå›¾åŠŸèƒ½ 
    * **analyze_datetime** - åˆ†æç½‘é¡µå‘å¸ƒ/æ›´æ–°æ—¶é—´
    * **search** - ç½‘ç»œæœç´¢åŠŸèƒ½
    * **health** - æœåŠ¡å¥åº·æ£€æŸ¥
    * **primer** - ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯

    ### ä½¿ç”¨æ–¹æ³•
    1. åœ¨ä¸‹æ–¹é€‰æ‹©è¦æµ‹è¯•çš„ API ç«¯ç‚¹
    2. å¡«å†™å¿…è¦çš„å‚æ•°
    3. ç‚¹å‡» "Try it out" æŒ‰é’®
    4. ç‚¹å‡» "Execute" æ‰§è¡Œè¯·æ±‚
    5. æŸ¥çœ‹è¿”å›ç»“æœ

    ### æ”¯æŒçš„ç½‘ç«™ç±»å‹
    - æ–°é—»ç½‘ç«™
    - åšå®¢æ–‡ç« 
    - ç»´åŸºç™¾ç§‘
    - çŸ¥ä¹æ–‡ç« 
    - æŠ€æœ¯æ–‡æ¡£
    - å¤§å¤šæ•°é™æ€ç½‘é¡µ

    """,
    contact={
        "name": "Visit API Support",
        "email": "support@visitapi.com",
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT",
    },
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class ReadUrlRequest(BaseModel):
    url: str
    include_images: Optional[bool] = False
    include_links: Optional[bool] = False
    max_content_length: Optional[int] = 10000

    model_config = {
        "json_schema_extra": {
            "example": {
                "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
                "include_images": True,
                "include_links": True,
                "max_content_length": 5000
            }
        }
    }

class ScreenshotRequest(BaseModel):
    url: str
    width: Optional[int] = 1280
    height: Optional[int] = 720
    full_page: Optional[bool] = False
    format: Optional[str] = "png"

    model_config = {
        "json_schema_extra": {
            "example": {
                "url": "https://www.example.com",
                "width": 1280,
                "height": 720,
                "full_page": False,
                "format": "png"
            }
        }
    }

class ParallelReadRequest(BaseModel):
    urls: List[str]
    include_images: Optional[bool] = False
    include_links: Optional[bool] = False
    max_content_length: Optional[int] = 10000

    model_config = {
        "json_schema_extra": {
            "example": {
                "urls": [
                    "https://en.wikipedia.org/wiki/Machine_learning",
                    "https://en.wikipedia.org/wiki/Deep_learning",
                    "https://en.wikipedia.org/wiki/Natural_language_processing"
                ],
                "include_images": False,
                "include_links": True,
                "max_content_length": 3000
            }
        }
    }

class SearchRequest(BaseModel):
    query: str
    max_results: Optional[int] = 10
    language: Optional[str] = "en"

    model_config = {
        "json_schema_extra": {
            "example": {
                "query": "artificial intelligence latest news",
                "max_results": 10,
                "language": "en"
            }
        }
    }

class DatetimeAnalysisRequest(BaseModel):
    url: str

    model_config = {
        "json_schema_extra": {
            "example": {
                "url": "https://news.ycombinator.com/item?id=123456"
            }
        }
    }

# å“åº”æ¨¡å‹
class PageContent(BaseModel):
    url: str
    title: Optional[str] = None
    content: Optional[str] = None
    clean_text: Optional[str] = None
    meta_info: Optional[Dict[str, Any]] = None
    images: Optional[List[Dict[str, str]]] = None
    links: Optional[List[Dict[str, str]]] = None
    extraction_status: str
    error: Optional[str] = None
    timestamp: str

class ScreenshotResponse(BaseModel):
    url: str
    screenshot_base64: Optional[str] = None
    width: int
    height: int
    format: str
    error: Optional[str] = None
    timestamp: str

class DatetimeInfo(BaseModel):
    url: str
    published_date: Optional[str] = None
    updated_date: Optional[str] = None
    detected_dates: Optional[List[str]] = None
    confidence: Optional[float] = None
    method: Optional[str] = None
    error: Optional[str] = None

class ApiResponse(BaseModel):
    code: int = 200
    log_id: str
    msg: Optional[str] = None
    data: Union[Dict[str, Any], List[Dict[str, Any]], Any]
    timestamp: str

# æ—¥å¿—ä¸­é—´ä»¶
@app.middleware("http")
async def log_requests(request, call_next):
    start_time = time.time()
    logger.info(f"Request started: {request.method} {request.url}")
    
    response = await call_next(request)
    
    process_time = time.time() - start_time
    logger.info(f"Request completed: {request.method} {request.url} - Status: {response.status_code} - Time: {process_time:.3f}s")
    
    return response

# å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global error handler caught: {str(exc)}")
    logger.error(traceback.format_exc())
    return JSONResponse(
        status_code=500,
        content={
            "code": 500,
            "log_id": datetime.now().strftime('%Y%m%d%H%M%S'),
            "msg": str(exc),
            "data": None,
            "timestamp": datetime.now(pytz.UTC).isoformat(),
            "traceback": traceback.format_exc() if logging.getLogger().level == logging.DEBUG else None
        }
    )

# å·¥å…·å‡½æ•°
def format_timestamp() -> str:
    """è¿”å›UTCæ—¶é—´æˆ³"""
    return datetime.now(pytz.UTC).isoformat()

def create_proxy_connector():
    """åˆ›å»ºä»£ç†è¿æ¥å™¨"""
    proxy_config = NETWORK_CONFIG.get("proxy", {})
    if not proxy_config.get("enabled", False):
        return None
        
    # æ„å»ºä»£ç†URL
    proxy_url = proxy_config.get("https") or proxy_config.get("http")
    if not proxy_url:
        return None
        
    # å¦‚æœéœ€è¦è®¤è¯
    auth = proxy_config.get("auth", {})
    if auth.get("username") and auth.get("password"):
        from urllib.parse import urlparse, urlunparse
        
        parsed = urlparse(proxy_url)
        # é‡æ„URLåŒ…å«è®¤è¯ä¿¡æ¯
        new_netloc = f"{auth['username']}:{auth['password']}@{parsed.netloc}"
        proxy_url = urlunparse(parsed._replace(netloc=new_netloc))
    
    return proxy_url

async def create_http_session():
    """åˆ›å»ºHTTPä¼šè¯ï¼Œæ”¯æŒä»£ç†"""
    timeout = aiohttp.ClientTimeout(
        total=NETWORK_CONFIG["timeout"]["total"], 
        connect=NETWORK_CONFIG["timeout"]["connect"]
    )
    
    proxy_url = create_proxy_connector()
    
    # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œç¦ç”¨è¯ä¹¦éªŒè¯ä»¥è§£å†³ä»£ç†SSLé—®é¢˜
    import ssl
    ssl_context = ssl.create_default_context()
    ssl_context.check_hostname = False
    ssl_context.verify_mode = ssl.CERT_NONE
    
    if proxy_url:
        logger.info(f"ä½¿ç”¨ä»£ç†: {proxy_url.split('@')[-1] if '@' in proxy_url else proxy_url}")
        # åˆ›å»ºè¿æ¥å™¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡
        connector = aiohttp.TCPConnector(
            ssl=ssl_context,  # ä½¿ç”¨è‡ªå®šä¹‰SSLä¸Šä¸‹æ–‡
            limit=100,
            limit_per_host=30
        )
        return aiohttp.ClientSession(
            timeout=timeout,
            connector=connector
        ), proxy_url
    else:
        # æ²¡æœ‰ä»£ç†æ—¶ä¹Ÿä½¿ç”¨ç›¸åŒçš„SSLé…ç½®ä»¥ä¿æŒä¸€è‡´æ€§
        connector = aiohttp.TCPConnector(
            ssl=ssl_context,
            limit=100,
            limit_per_host=30
        )
        return aiohttp.ClientSession(
            timeout=timeout,
            connector=connector
        ), None

async def process_soup_content(soup, url: str, include_images: bool, include_links: bool, max_length: int) -> PageContent:
    """å¤„ç†BeautifulSoupè§£æåçš„å†…å®¹"""
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):
        tag.decompose()
    
    # æå–æ ‡é¢˜
    title = soup.title.string.strip() if soup.title else None
    
    # æå–å…ƒæ•°æ®
    meta_info = {
        'description': soup.find('meta', {'name': 'description'})['content'] if soup.find('meta', {'name': 'description'}) else None,
        'keywords': soup.find('meta', {'name': 'keywords'})['content'] if soup.find('meta', {'name': 'keywords'}) else None,
        'author': soup.find('meta', {'name': 'author'})['content'] if soup.find('meta', {'name': 'author'}) else None,
        'language': soup.find('html', {'lang': True})['lang'] if soup.find('html', {'lang': True}) else None,
        'charset': soup.find('meta', {'charset': True})['charset'] if soup.find('meta', {'charset': True}) else None,
    }
            
    
    # æå–ä¸»è¦å†…å®¹
    content = ""
    
    # å°è¯•æŸ¥æ‰¾ä¸»è¦å†…å®¹åŒºåŸŸ
    main_selectors = [
        'main', 'article', '[role="main"]',
        '.content', '.main-content', '.article-content',
        '.post-content', '.entry-content', '#content'
    ]
    
    for selector in main_selectors:
        main_content = soup.select_one(selector)
        if main_content:
            content = main_content.get_text(separator=' ', strip=True)
            break
    
    # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œæå–bodyå†…å®¹
    if not content:
        body = soup.find('body')
        if body:
            content = body.get_text(separator=' ', strip=True)
    
    # æ¸…ç†å’Œé™åˆ¶å†…å®¹é•¿åº¦
    clean_text = ' '.join(content.split())
    if len(clean_text) > max_length:
        clean_text = clean_text[:max_length] + '...'
    
    # æå–å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    images = []
    if include_images:
        for img in soup.find_all('img', src=True):
            src = img['src']
            if not src.startswith(('http://', 'https://')):
                from urllib.parse import urljoin
                src = urljoin(url, src)
            images.append({
                'src': src,
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
    
    # æå–é“¾æ¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
    links = []
    if include_links:
        for link in soup.find_all('a', href=True):
            href = link['href']
            if not href.startswith(('http://', 'https://')):
                from urllib.parse import urljoin
                href = urljoin(url, href)
            links.append({
                'href': href,
                'text': link.get_text(strip=True),
                'title': link.get('title', '')
            })
    
    return PageContent(
        url=url,
        title=title,
        content=content[:max_length] if content else None,
        clean_text=clean_text,
        meta_info=meta_info,
        images=images[:20] if images else None,  # é™åˆ¶å›¾ç‰‡æ•°é‡
        links=links[:50] if links else None,    # é™åˆ¶é“¾æ¥æ•°é‡
        extraction_status='success',
        timestamp=format_timestamp()
    )

async def extract_content_with_requests(url: str, include_images: bool, include_links: bool, max_length: int) -> PageContent:
    """ä½¿ç”¨requestsåº“æå–å†…å®¹ï¼ˆé€‚ç”¨äºHTTPSä»£ç†ï¼‰"""
    import cloudscraper
    import asyncio

    logger.info(f"Using 'cloudscraper' library for {url}")
    
    # ä½¿ç”¨æ›´çœŸå®çš„æµè§ˆå™¨é…ç½®åˆ›å»ºscraper
    scraper = cloudscraper.create_scraper(
        browser={
            'browser': 'chrome',
            'platform': 'windows',
            'mobile': False
        }
    )
    
    proxy_config = NETWORK_CONFIG.get("proxy", {})
    proxy_url = proxy_config.get("https") or proxy_config.get("http")
    
    proxies = {
        'http': proxy_url,
        'https': proxy_url
    } if proxy_url else None

    max_retries = NETWORK_CONFIG["retries"]["max_attempts"]
    delay_factor = NETWORK_CONFIG["retries"]["delay_factor"]
    
    soup = None
    for attempt in range(max_retries):
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: scraper.get(
                    url,
                    proxies=proxies,
                    timeout=(NETWORK_CONFIG["timeout"]["connect"], NETWORK_CONFIG["timeout"]["total"])
                )
            )
            
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            break
            
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"Attempt {attempt + 1} with 'cloudscraper' failed for {url}: {str(e)}, retrying...")
                await asyncio.sleep(delay_factor * (attempt + 1))
                continue
            else:
                logger.error(f"All {max_retries} 'cloudscraper' attempts failed for {url}")
                raise e

    if soup is None:
        raise Exception("Failed to fetch content with 'cloudscraper' after all retries.")

    return await process_soup_content(soup, url, include_images, include_links, max_length)

async def extract_content_with_aiohttp(url: str, include_images: bool, include_links: bool, max_length: int) -> PageContent:
    """ä½¿ç”¨aiohttpæå–å†…å®¹ï¼ˆé€‚ç”¨äºHTTPæˆ–æ— ä»£ç†ï¼‰"""
    logger.info(f"Using 'aiohttp' library for {url}")
    headers = {
        'User-Agent': NETWORK_CONFIG["user_agent"],
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9,zh-CN,zh;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
    }
    
    max_retries = NETWORK_CONFIG["retries"]["max_attempts"]
    delay_factor = NETWORK_CONFIG["retries"]["delay_factor"]
    
    soup = None
    for attempt in range(max_retries):
        try:
            session, proxy_url = await create_http_session()
            async with session:
                request_kwargs = {
                    'headers': headers,
                    'allow_redirects': True
                }
                if proxy_url:
                    request_kwargs['proxy'] = proxy_url
                
                async with session.get(url, **request_kwargs) as response:
                    response.raise_for_status() # å¦‚æœçŠ¶æ€ç ä¸æ˜¯2xxï¼Œä¼šæŠ›å‡ºClientResponseError
                    
                    text = await response.text()
                    soup = BeautifulSoup(text, 'html.parser')
                    break  # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                        
        except (aiohttp.ServerTimeoutError, aiohttp.ClientConnectorError, aiohttp.ClientError, asyncio.TimeoutError) as e:
            if attempt < max_retries - 1:
                logger.warning(f"Attempt {attempt + 1} with 'aiohttp' failed for {url}: {str(e)}, retrying...")
                await asyncio.sleep(delay_factor * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                continue
            else:
                logger.error(f"All {max_retries} 'aiohttp' attempts failed for {url}")
                raise e # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œç”±ä¸Šå±‚å¤„ç†

    if soup is None:
        raise Exception("Failed to fetch content with 'aiohttp' after all retries.")
    
    return await process_soup_content(soup, url, include_images, include_links, max_length)


async def extract_clean_content(url: str, include_images: bool = False, include_links: bool = False, max_length: int = 10000) -> PageContent:
    """æå–ç½‘é¡µçš„å¹²å‡€å†…å®¹"""
    logger.debug(f"Starting content extraction for URL: {url}")
    
    if not url or not url.startswith(('http://', 'https://')):
        return PageContent(
            url=url,
            extraction_status='error',
            error='Invalid URL format',
            timestamp=format_timestamp()
        )
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºHTTPSä¸”å¯ç”¨äº†ä»£ç†
        proxy_config = NETWORK_CONFIG.get("proxy", {})
        proxy_enabled = proxy_config.get("enabled", False)
        is_https = url.startswith('https://')
        
        if is_https and proxy_enabled:
            # å¯¹äºHTTPS + ä»£ç†ï¼Œä½¿ç”¨requestsåº“ï¼ˆæ›´å¯é ï¼‰
            return await extract_content_with_requests(url, include_images, include_links, max_length)
        else:
            # å¯¹äºHTTPæˆ–æ— ä»£ç†ï¼Œä½¿ç”¨aiohttp
            return await extract_content_with_aiohttp(url, include_images, include_links, max_length)
            
    except Exception as e:
        logger.error(f"Error extracting content from {url}: {str(e)}")
        logger.error(traceback.format_exc())
        return PageContent(
            url=url,
            extraction_status='error',
            error=str(e),
            timestamp=format_timestamp()
        )

async def capture_screenshot(url: str, width: int = 1280, height: int = 720, full_page: bool = False, format: str = "png") -> ScreenshotResponse:
    """æ¨¡æ‹Ÿæˆªå›¾åŠŸèƒ½ï¼ˆå®é™…éœ€è¦æµè§ˆå™¨å¼•æ“ï¼‰"""
    logger.debug(f"Screenshot request for URL: {url}")
    
    # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éœ€è¦ä½¿ç”¨ Playwright æˆ– Selenium
    # ç”±äºæ²¡æœ‰æµè§ˆå™¨å¼•æ“ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
    
    return ScreenshotResponse(
        url=url,
        screenshot_base64=None,  # å®é™…å®ç°éœ€è¦è¿”å›base64ç¼–ç çš„å›¾ç‰‡
        width=width,
        height=height,
        format=format,
        error="Screenshot service not implemented - requires browser engine",
        timestamp=format_timestamp()
    )

async def analyze_page_datetime(url: str) -> DatetimeInfo:
    """åˆ†æç½‘é¡µçš„å‘å¸ƒ/æ›´æ–°æ—¶é—´"""
    logger.debug(f"Analyzing datetime for URL: {url}")
    
    try:
        timeout = aiohttp.ClientTimeout(total=45, connect=15)  # å¢åŠ è¶…æ—¶æ—¶é—´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        session, proxy_url = await create_http_session()
        async with session:
            request_kwargs = {
                'headers': headers
            }
            if proxy_url:
                request_kwargs['proxy'] = proxy_url
                
            async with session.get(url, **request_kwargs) as response:
                if response.status != 200:
                    return DatetimeInfo(
                        url=url,
                        error=f'HTTP {response.status}'
                    )
                
                text = await response.text()
                soup = BeautifulSoup(text, 'html.parser')
                
                # æŸ¥æ‰¾å„ç§æ—¶é—´æ ‡è®°
                published_date = None
                updated_date = None
                detected_dates = []
                
                # æŸ¥æ‰¾å…ƒæ•°æ®ä¸­çš„æ—¶é—´
                date_metas = [
                    'article:published_time', 'article:modified_time',
                    'datePublished', 'dateModified', 'pubdate'
                ]
                
                for meta_name in date_metas:
                    meta = soup.find('meta', {'property': meta_name}) or soup.find('meta', {'name': meta_name})
                    if meta and meta.get('content'):
                        date_str = meta['content']
                        detected_dates.append(date_str)
                        if 'published' in meta_name.lower() and not published_date:
                            published_date = date_str
                        elif 'modified' in meta_name.lower() and not updated_date:
                            updated_date = date_str
                
                # æŸ¥æ‰¾timeæ ‡ç­¾
                for time_tag in soup.find_all('time'):
                    datetime_attr = time_tag.get('datetime')
                    if datetime_attr:
                        detected_dates.append(datetime_attr)
                        if not published_date:
                            published_date = datetime_attr
                
                # ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—
                confidence = min(1.0, len(detected_dates) * 0.3)
                
                return DatetimeInfo(
                    url=url,
                    published_date=published_date,
                    updated_date=updated_date,
                    detected_dates=detected_dates,
                    confidence=confidence,
                    method='meta_and_time_tags'
                )
                
    except Exception as e:
        logger.error(f"Error analyzing datetime for {url}: {str(e)}")
        return DatetimeInfo(
            url=url,
            error=str(e)
        )

# API ç«¯ç‚¹

@app.get("/test", 
         response_class=HTMLResponse,
         summary="æµ‹è¯•é¡µé¢",
         description="æä¾›ä¸€ä¸ªäº¤äº’å¼çš„æµ‹è¯•ç•Œé¢ï¼Œå¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æµ‹è¯•æ‰€æœ‰ API åŠŸèƒ½",
         tags=["æµ‹è¯•å·¥å…·"])
async def test_page():
    """è¿”å›æµ‹è¯•é¡µé¢"""
    try:
        with open("test.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    except FileNotFoundError:
        return HTMLResponse("""
        <html>
            <body style="font-family: Arial; text-align: center; padding: 50px;">
                <h1>æµ‹è¯•é¡µé¢ä¸å­˜åœ¨</h1>
                <p>è¯·ç¡®ä¿ test.html æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­</p>
                <p><a href="/docs">ç‚¹å‡»è¿™é‡Œè®¿é—® API æ–‡æ¡£</a></p>
            </body>
        </html>
        """)

@app.get("/",
         response_class=HTMLResponse,
         summary="æœåŠ¡é¦–é¡µ",
         description="Visit API æœåŠ¡çš„æ¬¢è¿é¡µé¢ï¼Œæä¾›æœåŠ¡æ¦‚è§ˆå’Œå¿«é€Ÿé“¾æ¥",
         tags=["ç³»ç»Ÿ"])
async def root():
    """æ ¹ç«¯ç‚¹ - æœåŠ¡ä¿¡æ¯"""
    return HTMLResponse("""
    <!DOCTYPE html>
    <html lang="zh-CN">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Visit API - ç½‘é¡µè®¿é—®æœåŠ¡</title>
        <style>
            body {
                font-family: 'Segoe UI', Arial, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                margin: 0;
                padding: 50px 20px;
                color: white;
                text-align: center;
                min-height: 100vh;
                display: flex;
                flex-direction: column;
                justify-content: center;
            }
            .container {
                max-width: 800px;
                margin: 0 auto;
                background: rgba(255,255,255,0.1);
                padding: 50px;
                border-radius: 20px;
                backdrop-filter: blur(10px);
                box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            }
            h1 {
                font-size: 3em;
                margin-bottom: 20px;
                text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            }
            .subtitle {
                font-size: 1.3em;
                margin-bottom: 40px;
                opacity: 0.9;
            }
            .features {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                gap: 20px;
                margin: 40px 0;
            }
            .feature {
                background: rgba(255,255,255,0.1);
                padding: 20px;
                border-radius: 10px;
                border: 1px solid rgba(255,255,255,0.2);
            }
            .links {
                margin-top: 40px;
            }
            .btn {
                display: inline-block;
                background: rgba(255,255,255,0.2);
                color: white;
                padding: 15px 30px;
                margin: 10px;
                border-radius: 50px;
                text-decoration: none;
                border: 2px solid rgba(255,255,255,0.3);
                transition: all 0.3s;
                font-weight: 500;
            }
            .btn:hover {
                background: rgba(255,255,255,0.3);
                transform: translateY(-2px);
                box-shadow: 0 10px 20px rgba(0,0,0,0.2);
            }
            .status {
                background: rgba(76, 175, 80, 0.2);
                padding: 10px 20px;
                border-radius: 20px;
                display: inline-block;
                margin-bottom: 20px;
                border: 1px solid rgba(76, 175, 80, 0.5);
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="status">ğŸŸ¢ æœåŠ¡è¿è¡Œä¸­</div>
            <h1>ğŸŒ Visit API</h1>
            <p class="subtitle">ç½‘é¡µè®¿é—®æœåŠ¡ - å†…å®¹æå–ã€åˆ†æå’Œå¤„ç†</p>
            
            <div class="features">
                <div class="feature">
                    <h3>ğŸ” å†…å®¹æå–</h3>
                    <p>æ™ºèƒ½æå–ç½‘é¡µæ ¸å¿ƒå†…å®¹</p>
                </div>
                <div class="feature">
                    <h3>âš¡ å¹¶è¡Œå¤„ç†</h3>
                    <p>åŒæ—¶å¤„ç†å¤šä¸ªç½‘é¡µ</p>
                </div>
                <div class="feature">
                    <h3>ğŸ•’ æ—¶é—´åˆ†æ</h3>
                    <p>åˆ†æç½‘é¡µæ—¶é—´ä¿¡æ¯</p>
                </div>
                <div class="feature">
                    <h3>ğŸ“Š æ•°æ®ç»“æ„åŒ–</h3>
                    <p>è¿”å›ç»“æ„åŒ–çš„æ•°æ®</p>
                </div>
            </div>
            
            <div class="links">
                <a href="/test" class="btn">ğŸ§ª äº¤äº’å¼æµ‹è¯•</a>
                <a href="/docs" class="btn">ğŸ“– API æ–‡æ¡£</a>
                <a href="/health" class="btn">ğŸ’Š å¥åº·æ£€æŸ¥</a>
                <a href="/primer" class="btn">â„¹ï¸ ç³»ç»Ÿä¿¡æ¯</a>
            </div>
            
            <p style="margin-top: 40px; opacity: 0.7;">
                Visit API v1.0.0 | åŸºäº Jina MCP æ¥å£è®¾è®¡
            </p>
        </div>
    </body>
    </html>
    """)

@app.post("/read_url", 
          response_model=ApiResponse,
          summary="æå–ç½‘é¡µå†…å®¹",
          description="""
          æå–æŒ‡å®šç½‘é¡µçš„å¹²å‡€å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ­£æ–‡ã€å…ƒæ•°æ®ç­‰ä¿¡æ¯ã€‚
          
          **æ”¯æŒçš„åŠŸèƒ½ï¼š**
          - è‡ªåŠ¨è¯†åˆ«ä¸»è¦å†…å®¹åŒºåŸŸ
          - æ¸…ç†æ— ç”¨çš„æ ‡ç­¾å’Œè„šæœ¬
          - æå–å›¾ç‰‡å’Œé“¾æ¥ï¼ˆå¯é€‰ï¼‰
          - æ”¯æŒå¤šç§å­—ç¬¦ç¼–ç 
          - å¤„ç†å„ç§ç½‘ç«™ç±»å‹
          
          **ç¤ºä¾‹ç½‘ç«™ï¼š**
          - https://en.wikipedia.org/wiki/Artificial_intelligence
          - https://www.zhihu.com/question/xxxxx
          - https://news.ycombinator.com/item?id=xxxxx
          """,
          tags=["å†…å®¹æå–"])
async def read_url(request: ReadUrlRequest):
    """æå–ç½‘é¡µå†…å®¹"""
    logger.info(f"Reading URL: {request.url}")
    
    try:
        content = await extract_clean_content(
            url=request.url,
            include_images=request.include_images,
            include_links=request.include_links,
            max_length=request.max_content_length
        )
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Success",
            data=content.model_dump(),
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error reading URL {request.url}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/parallel_read", 
          response_model=ApiResponse,
          summary="å¹¶è¡Œè¯»å–å¤šä¸ªç½‘é¡µ",
          description="""
          åŒæ—¶è¯»å–å¤šä¸ªç½‘é¡µçš„å†…å®¹ï¼Œé€‚ç”¨äºéœ€è¦å¿«é€Ÿè·å–å¤šä¸ªé¡µé¢ä¿¡æ¯çš„åœºæ™¯ã€‚
          
          **ç‰¹ç‚¹ï¼š**
          - å¹¶å‘å¤„ç†ï¼Œæ˜¾è‘—æé«˜æ•ˆç‡
          - è‡ªåŠ¨å¤„ç†å¤±è´¥çš„è¯·æ±‚
          - æ”¯æŒæœ€å¤šåŒæ—¶å¤„ç† 10 ä¸ªç½‘é¡µ
          - è¿”å›æ‰€æœ‰ç»“æœï¼ˆåŒ…æ‹¬å¤±è´¥çš„ï¼‰
          
          **ä½¿ç”¨åœºæ™¯ï¼š**
          - æ–°é—»èšåˆ
          - å†…å®¹æ¯”è¾ƒ
          - æ‰¹é‡æ•°æ®æ”¶é›†
          """,
          tags=["å†…å®¹æå–"])
async def parallel_read(request: ParallelReadRequest):
    """å¹¶è¡Œè¯»å–å¤šä¸ªç½‘é¡µ"""
    logger.info(f"Parallel reading {len(request.urls)} URLs")
    
    try:
        # é™åˆ¶æœ€å¤§å¹¶å‘æ•°
        max_parallel = CONTENT_CONFIG["parallel_limit"]
        if len(request.urls) > max_parallel:
            raise HTTPException(
                status_code=400, 
                detail=f"æœ€å¤šæ”¯æŒåŒæ—¶è¯»å– {max_parallel} ä¸ªç½‘é¡µ"
            )
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for url in request.urls:
            task = extract_clean_content(
                url=url,
                include_images=request.include_images,
                include_links=request.include_links,
                max_length=request.max_content_length
            )
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append({
                    "url": request.urls[i],
                    "extraction_status": "error",
                    "error": str(result),
                    "timestamp": format_timestamp()
                })
            else:
                processed_results.append(result.model_dump())
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Parallel read completed",
            data=processed_results,
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error in parallel read: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/screenshot", 
          response_model=ApiResponse,
          summary="ç½‘é¡µæˆªå›¾",
          description="""
          æ•è·ç½‘é¡µçš„æˆªå›¾å›¾åƒã€‚
          
          **æ³¨æ„ï¼š** å½“å‰ä¸ºæ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éƒ¨ç½²éœ€è¦é›†æˆæµè§ˆå™¨å¼•æ“ï¼ˆå¦‚ Playwright æˆ– Seleniumï¼‰ã€‚
          
          **å‚æ•°è¯´æ˜ï¼š**
          - width/height: æˆªå›¾å°ºå¯¸
          - full_page: æ˜¯å¦æˆªå–æ•´ä¸ªé¡µé¢
          - format: å›¾ç‰‡æ ¼å¼ (png/jpeg)
          
          **å®é™…å®ç°éœ€è¦ï¼š**
          ```bash
          pip install playwright
          playwright install chromium
          ```
          """,
          tags=["å›¾åƒå¤„ç†"])
async def screenshot(request: ScreenshotRequest):
    """ç½‘é¡µæˆªå›¾ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"Screenshot request for: {request.url}")
    
    try:
        screenshot_result = await capture_screenshot(
            url=request.url,
            width=request.width,
            height=request.height,
            full_page=request.full_page,
            format=request.format
        )
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Screenshot completed (mock)",
            data=screenshot_result.model_dump(),
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error capturing screenshot: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/analyze_datetime", 
          response_model=ApiResponse,
          summary="åˆ†æç½‘é¡µæ—¶é—´ä¿¡æ¯",
          description="""
          åˆ†æç½‘é¡µçš„å‘å¸ƒæ—¶é—´å’Œæ›´æ–°æ—¶é—´ã€‚
          
          **åˆ†ææ–¹æ³•ï¼š**
          - æå– HTML meta æ ‡ç­¾ä¸­çš„æ—¶é—´ä¿¡æ¯
          - æŸ¥æ‰¾ `<time>` æ ‡ç­¾çš„ datetime å±æ€§
          - æ£€æµ‹å¸¸è§çš„æ—¶é—´æ ¼å¼
          - è®¡ç®—æ£€æµ‹ç½®ä¿¡åº¦
          
          **é€‚ç”¨ç½‘ç«™ï¼š**
          - æ–°é—»ç½‘ç«™
          - åšå®¢æ–‡ç« 
          - æŠ€æœ¯æ–‡æ¡£
          - ç¤¾äº¤åª’ä½“å¸–å­
          """,
          tags=["æ•°æ®åˆ†æ"])
async def analyze_datetime(request: DatetimeAnalysisRequest):
    """åˆ†æç½‘é¡µçš„å‘å¸ƒ/æ›´æ–°æ—¶é—´"""
    logger.info(f"Analyzing datetime for: {request.url}")
    
    try:
        datetime_info = await analyze_page_datetime(request.url)
        
        return ApiResponse(
            code=200,
            log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
            msg="Datetime analysis completed",
            data=datetime_info.model_dump(),
            timestamp=format_timestamp()
        )
        
    except Exception as e:
        logger.error(f"Error analyzing datetime: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/search", 
          response_model=ApiResponse,
          summary="ç½‘ç»œæœç´¢",
          description="""
          æ‰§è¡Œç½‘ç»œæœç´¢å¹¶è¿”å›ç»“æœã€‚
          
          **æ³¨æ„ï¼š** å½“å‰ä¸ºæ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éƒ¨ç½²éœ€è¦é›†æˆæœç´¢å¼•æ“ APIã€‚
          
          **å¯é›†æˆçš„æœç´¢å¼•æ“ï¼š**
          - DuckDuckGo (å…è´¹)
          - Google Custom Search API
          - Bing Search API
          - SerpAPI
          
          **å®é™…å®ç°éœ€è¦ï¼š**
          ```bash
          pip install duckduckgo-search
          ```
          """,
          tags=["æœç´¢"])
async def search_web(request: SearchRequest):
    """ç½‘ç»œæœç´¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
    logger.info(f"Search request for: {request.query}")
    
    # è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éœ€è¦é›†æˆæœç´¢å¼•æ“API
    mock_results = [
        {
            "title": f"æœç´¢ç»“æœ 1: {request.query}",
            "url": "https://example.com/search-result-1",
            "snippet": f"è¿™æ˜¯å…³äº '{request.query}' çš„ç¬¬ä¸€ä¸ªæœç´¢ç»“æœæ‘˜è¦...",
            "rank": 1
        },
        {
            "title": f"æœç´¢ç»“æœ 2: {request.query}",
            "url": "https://example.com/search-result-2", 
            "snippet": f"è¿™æ˜¯å…³äº '{request.query}' çš„ç¬¬äºŒä¸ªæœç´¢ç»“æœæ‘˜è¦...",
            "rank": 2
        }
    ]
    
    return ApiResponse(
        code=200,
        log_id=datetime.now().strftime('%Y%m%d%H%M%S'),
        msg="Search completed (mock)",
        data={
            "query": request.query,
            "results": mock_results[:request.max_results],
            "total_results": len(mock_results),
            "language": request.language
        },
        timestamp=format_timestamp()
    )

@app.get("/health",
         summary="å¥åº·æ£€æŸ¥",
         description="æ£€æŸ¥æœåŠ¡çš„è¿è¡ŒçŠ¶æ€å’Œç³»ç»Ÿå¥åº·çŠ¶å†µ",
         tags=["ç³»ç»Ÿ"])
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    logger.info("Health check endpoint accessed")
    
    # æ£€æŸ¥ä»£ç†çŠ¶æ€
    proxy_config = NETWORK_CONFIG.get("proxy", {})
    proxy_enabled = proxy_config.get("enabled", False)
    proxy_url = None
    if proxy_enabled:
        proxy_url = proxy_config.get("https") or proxy_config.get("http")
    
    health_status = {
        "status": "healthy",
        "service": "Visit API",
        "version": "1.0.0",
        "timestamp": format_timestamp(),
        "proxy_enabled": proxy_enabled,
        "proxy_url": proxy_url if proxy_enabled else None,
        "checks": {
            "logging": "ok",
            "memory": "ok",
            "disk_space": "ok",
            "proxy": "enabled" if proxy_enabled else "disabled"
        }
    }
    
    return health_status

@app.get("/primer",
         summary="ç³»ç»Ÿä¿¡æ¯",
         description="""
         æä¾›ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ—¶é—´æˆ³ã€ç½‘ç»œä¿¡æ¯ã€æœåŠ¡èƒ½åŠ›ç­‰ã€‚
         
         **è¿”å›ä¿¡æ¯ï¼š**
         - å½“å‰æ—¶é—´æˆ³å’Œæ—¶åŒº
         - ç³»ç»Ÿä¸»æœºåå’ŒIP
         - å¹³å°å’ŒPythonç‰ˆæœ¬
         - æœåŠ¡ç‰ˆæœ¬å’Œèƒ½åŠ›åˆ—è¡¨
         """,
         tags=["ç³»ç»Ÿ"])
async def primer():
    """æä¾›ç³»ç»Ÿä¸Šä¸‹æ–‡ä¿¡æ¯"""
    import socket
    import platform
    
    try:
        hostname = socket.gethostname()
        local_ip = socket.gethostbyname(hostname)
    except:
        hostname = "unknown"
        local_ip = "unknown"
    
    return {
        "timestamp": format_timestamp(),
        "timezone": str(pytz.UTC),
        "system_info": {
            "hostname": hostname,
            "local_ip": local_ip,
            "platform": platform.system(),
            "python_version": platform.python_version()
        },
        "client_context": {
            "service": "Visit API",
            "version": "1.0.0",
            "capabilities": [
                "read_url", "parallel_read", "screenshot", 
                "analyze_datetime", "search", "health_check"
            ]
        }
    }

if __name__ == "__main__":
    import uvicorn
    
    logger.info("Starting Visit API server...")
    logger.info(f"Log file: {log_filename}")
    
    uvicorn.run(
        app,
        host=SERVICE_CONFIG["host"],
        port=SERVICE_CONFIG["port"],
        log_level="info"
    )
